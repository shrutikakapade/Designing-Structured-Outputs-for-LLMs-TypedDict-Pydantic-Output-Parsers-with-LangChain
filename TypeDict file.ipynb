{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 15001,
     "status": "ok",
     "timestamp": 1770457488855,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "Zwio4i44D_j1",
    "outputId": "9b513fa5-8314-49ea-b3fc-e2515b1ffe92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.8)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.14.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (2.5.0)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.8)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.16.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (2.12.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.13.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
      "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain_openai\n",
      "Successfully installed langchain_openai-1.1.7\n",
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.61.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.2.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.12.3)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.12.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.47.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.32.4)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (6.0.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.6.2)\n",
      "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: filetype, langchain_google_genai\n",
      "Successfully installed filetype-1.2.0 langchain_google_genai-4.2.0\n"
     ]
    }
   ],
   "source": [
    "# Install LangChain library\n",
    "# LangChain helps us connect Large Language Models (LLMs) with prompts, chains, and tools\n",
    "# LangChain → Framework to build AI applications using LLMs\n",
    "!pip install langchain\n",
    "\n",
    "# Install OpenAI wrapper (optional, future-ready)\n",
    "# Useful if we switch from Gemini to OpenAI models later\n",
    "# Wrapper → A layer that connects external models with LangChain\n",
    "!pip install langchain_openai\n",
    "\n",
    "# Install Google Gemini integration for LangChain\n",
    "# Enables us to use Google’s Gemini models inside LangChain\n",
    "!pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770457488866,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "qqNTPjmvEYIM"
   },
   "outputs": [],
   "source": [
    "# Import OS module\n",
    "# Used to interact with environment variables securely\n",
    "# os → Python module to manage system-level operations like API keys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1770457489726,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "wSjVGICyEYE4"
   },
   "outputs": [],
   "source": [
    "# Import userdata from Google Colab\n",
    "# This allows us to safely access stored secrets\n",
    "from google.colab import userdata\n",
    "\n",
    "gem = userdata.get('Gemini') # Fetch Gemini API key from Colab secrets\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=gem\n",
    "\n",
    "# Store the API key as an environment variable\n",
    "# LangChain automatically reads API keys from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 24540,
     "status": "ok",
     "timestamp": 1770457514268,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "qNM1eL4OEYCd"
   },
   "outputs": [],
   "source": [
    "# Import Gemini text and chat models from LangChain\n",
    "from langchain_google_genai import GoogleGenerativeAI ,ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1770457514418,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "Sr7QAW_CEYAH"
   },
   "outputs": [],
   "source": [
    "\n",
    "model =ChatGoogleGenerativeAI(model = \"gemini-3-flash-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoAN_dH3b36_"
   },
   "source": [
    "Note : When using TypedDict in LangChain, always use a chat-based LLM model. Chat models are optimized for structured outputs, schema enforcement, and reliable JSON-like responses. Regular LLMs generate free-form text and may not follow strict data structures.\n",
    "\n",
    "Rule:\n",
    "TypedDict → Chat Model → Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1770457514433,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "X8vNf9kOEX9n"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATWB2l_Acr3a"
   },
   "source": [
    "We mention TypedDict from the typing module to define the expected structure of a dictionary with specific keys and value types.\n",
    "\n",
    "It helps:\n",
    "\n",
    "- Enforce structured data format\n",
    "\n",
    "- Improve code readability\n",
    "\n",
    "- Enable type checking\n",
    "\n",
    "- Ensure LLM outputs match a fixed schema when used with chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1770457514448,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "Q18bX62gEX7S"
   },
   "outputs": [],
   "source": [
    "# we are going to create our own schema (template for our data format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770457514469,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "CHojlyHAEX4-"
   },
   "outputs": [],
   "source": [
    "class DataFormat(TypedDict):\n",
    "   summary:str\n",
    "   experience:int\n",
    "\n",
    "   # This defines a structured dictionary format where summary must be a string and experience must be an integer, ensuring consistent and type-safe data output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1770457514516,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "1QutVLxOEX2l"
   },
   "outputs": [],
   "source": [
    "#example\n",
    "#obj = DataFormat(summary =\"Shrutika\",experience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 9849,
     "status": "ok",
     "timestamp": 1770457524367,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "cveao5xWEXxw"
   },
   "outputs": [],
   "source": [
    "response = model.invoke(\"i am having a 10 years of experience in opencv, agentic ai, ml, and dl, And i love to play cricket, skating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1770457524447,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "nZrHgNB-EXnF",
    "outputId": "7d05efdd-db6d-436f-f290-bf886ebac24d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'That is an incredibly powerful combination of skills! With 10 years in the field, you’ve likely seen the evolution from traditional feature descriptors (like SIFT/SURF in OpenCV) to the current era of Foundation Models and Autonomous Agents.\\n\\nSince you have such a deep technical background and active hobbies, there is a lot of \"cross-pollination\" we could talk about. Here are a few ways your world might intersect:\\n\\n### 1. The Professional Edge: From Vision to Agents\\n*   **The Transition:** Having a decade of experience means you likely started with C++/Python OpenCV for image processing and moved into CNNs/Transformers. Now, integrating **Agentic AI** means you’re probably looking at how LLMs can use vision tools to reason about the physical world.\\n*   **Agentic Vision:** Are you building systems where an agent uses OpenCV as a \"tool\" to perform specific inspections or spatial reasoning? That is one of the most exciting frontiers in AI right now.\\n\\n### 2. The Intersection of Hobbies and Tech\\nWith your expertise, you probably can\\'t help but look at your hobbies through a \"data lens\":\\n\\n*   **Cricket Analytics:** \\n    *   **OpenCV + DL:** You could build a ball-tracking system or a \"Hawk-eye\" clone using a simple GoPro and your own custom-trained YOLO models.\\n    *   **Agentic AI:** Imagine a \"Coach Agent\" that analyzes your batting stance via video feed (Pose Estimation) and provides real-time feedback on your footwork or bat swing.\\n*   **Skating Performance:** \\n    *   **Pose Estimation:** Using MediaPipe or AlphaPose to track your center of gravity while skating. \\n    *   **Edge AI:** You could potentially build a wearable or a camera system that detects a fall or analyzes the fluidity of your strides.\\n\\n### 3. Career Perspective\\nWith 10 years of experience, you are likely in a Senior, Lead, or Architect position. At this stage:\\n*   **Agentic AI** is the \"force multiplier.\" It’s no longer just about building the model; it’s about building the system that *uses* the model.\\n*   **OpenCV** remains the \"Swiss Army Knife\" for pre-processing data before it hits the high-compute DL models.\\n\\n---\\n\\n**I’d love to hear more about what you\\'re currently focused on:**\\n\\n1.  Are you building **Multi-Agent Systems** (like using CrewAI or AutoGen) for computer vision tasks?\\n2.  What\\'s your favorite cricket format? (Are you a fast bowler or a batsman?)\\n3.  In skating, are you into inline, quad, or ice skating?\\n\\nIt\\'s a pleasure to connect with a veteran in the AI space!',\n",
       "  'extras': {'signature': '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'}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770457524451,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "1o4mDKKLI-5D"
   },
   "outputs": [],
   "source": [
    "fm = model.with_structured_output(DataFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3622,
     "status": "ok",
     "timestamp": 1770457528074,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "8Mvy28FQI-2r"
   },
   "outputs": [],
   "source": [
    "response = fm.invoke(\"i am having a 10 years of experience in opencv, agentic ai, ml, and dl, And i love to play cricket, skating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1770457528085,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "erFZanatI-0V",
    "outputId": "9d2724fe-481c-4ded-a938-74d24ebde985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Experienced professional with 10 years of expertise in OpenCV, Agentic AI, Machine Learning, and Deep Learning, with a personal interest in cricket and skating.',\n",
       " 'experience': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1770457528090,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "-uAY220tI-x_",
    "outputId": "e1db46de-5569-4960-fd2f-6216d8e45c8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Experienced professional with 10 years of expertise in OpenCV, Agentic AI, Machine Learning, and Deep Learning, with a personal interest in cricket and skating.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1770457528106,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "ZBQgMW1HI-va",
    "outputId": "e0636a57-c054-4000-fbb0-3cd31245d5a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"experience\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1770457528108,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "ZPAJKTarI-s3"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "# Optional means a variable may have a value or may be None, allowing that field to be missing or unknown without causing an error.\n",
    "\n",
    "class DataFormat(TypedDict):\n",
    "   summary:str\n",
    "   experience:Optional[int]\n",
    "   skills :list[str]\n",
    "   hobbies : list[str]\n",
    "\n",
    "   # This defines a typed dictionary schema where summary is required, experience is optional (can be an integer or None), and skills and hobbies must be lists of strings, ensuring structured and flexible data output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770457528129,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "n2c0cWbMNCGM"
   },
   "outputs": [],
   "source": [
    "fm = model.with_structured_output(DataFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 10074,
     "status": "ok",
     "timestamp": 1770457538210,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "SXFexIz1NFZe"
   },
   "outputs": [],
   "source": [
    "response2 = fm.invoke(\"i am having a experience in opencv, agentic ai, ml, and dl, And i love to play cricket, skating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1770457538223,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "Ybu3RgZNNJFk",
    "outputId": "a5d0ef2b-4ff4-4f03-acce-57f68a17b747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'A professional with experience in computer vision and artificial intelligence, specializing in OpenCV, Agentic AI, Machine Learning, and Deep Learning.',\n",
       " 'experience': 1,\n",
       " 'skills': ['OpenCV', 'Agentic AI', 'ML', 'DL'],\n",
       " 'hobbies': ['Cricket', 'Skating']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1770457538226,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "sNv6HRMTNK_B"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "# Annotated is used to attach extra metadata or instructions to a type, without changing the type itself—commonly used to give LLMs or frameworks additional guidance on how to interpret or validate a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770457538229,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "4SoX-HwTOT2M"
   },
   "outputs": [],
   "source": [
    "class DataFormat1(TypedDict):\n",
    "  su:Annotated[str,\"give the summary of the resume\"]\n",
    "  ex:Annotated[Optional[int], \"if the experience is there you return the experience or else return NA\"]\n",
    "\n",
    "# This defines a structured output schema where su is a string summary of the resume and ex is an optional integer experience field, with Annotated providing instructions to guide the LLM on how each field should be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1770457538241,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "8MrZYBmAOTyz"
   },
   "outputs": [],
   "source": [
    "fm3 = model.with_structured_output(DataFormat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 18218,
     "status": "ok",
     "timestamp": 1770457556461,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "AHCcTwHvOTwf"
   },
   "outputs": [],
   "source": [
    "response2 = fm3.invoke(\"i am having a experience in opencv, agentic ai, ml, and dl, And i love to play cricket, skating etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1770457556479,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "lm-0JSGtOTuA",
    "outputId": "c7cdd581-900b-46d9-964d-d22ac4b56727"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'su': 'opencv, agentic ai, ml, dl, cricket, skating', 'ex': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2  #work perfectly on new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5153,
     "status": "ok",
     "timestamp": 1770457561630,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "wIICf802OTrd",
    "outputId": "bb3395e0-40db-4f5e-d52e-dd5339d3752e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pymupdf\n",
    "\n",
    "# !pip install --upgrade pymupdf installs or updates PyMuPDF, a library used for extracting and processing text and content from PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770458520313,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "AabtefdufOeJ"
   },
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "doc = pymupdf.open(\"/content/Data_Analysis_shrutika_kapade_resume_Final.pdf\")\n",
    "\n",
    "# This code imports the PyMuPDF library and opens the specified PDF file(resume), loading it into memory for text extraction and document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1770458524142,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "Oh3IEDJDhwUg",
    "outputId": "80e4ef98-557b-4a4d-ca1e-da98429f0ffd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>pymupdf.Document</b><br/>def __init__(filename=None, stream=None, filetype=None, rect=None, width=0, height=0, fontsize=11)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pymupdf/__init__.py</a>&lt;no docstring&gt;</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 2839);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "pymupdf.Document"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1770458529117,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "LtkxnUq8iMKn"
   },
   "outputs": [],
   "source": [
    "text =\" \"\n",
    "for page in doc:\n",
    "  text = text+page.get_text()\n",
    "# This code iterates through each page of the document, extracts the text from every page, and concatenates it into a single string for complete document text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1770458532493,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "M-4AZn-riQQ2",
    "outputId": "928e1f3d-8cde-4a1b-fd08-9a6acb4f6157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHRUTIKA KAPADE \n",
      "+91 8624929322 | shrutikakapade@gmail.com | LinkedIn | GitHub \n",
      "Professional Summary \n",
      "Data Scientist skilled in Python, SQL, EDA, Machine Learning, and Data Visualisation with hands-on experience in end-to-end \n",
      "analytics workflows. Completed practical projects involving data cleaning, feature engineering, and predictive modelling \n",
      "using standard ML techniques. Currently working as an AI Agent Intern and Data Science and GenAI Intern at Innomatics \n",
      "Research Labs, contributing to workflow automation, data validation, and process improvement. Focused on applying data-\n",
      "driven problem-solving to deliver clear insights and support effective business decision-making. \n",
      "Education \n",
      "Bachelor of Technology (B. Tech) Data Science \n",
      "Nov 2022 – Jun 2025 \n",
      "G.H. Raisoni College of Engineering and Management, Jalgaon. CGPA: 8.71 \n",
      "Technical Skills \n",
      "    Language:  \n",
      " \n",
      "Python (Pandas, NumPy, Scikit-learn), SQL (Window Functions, CTEs, Optimisations).  \n",
      "    BI Tools:  \n",
      " \n",
      "Power BI (Advanced DAX, Power Query), Excel (Power Pivot, VLOOKUP). \n",
      "    Data Engineering:  \n",
      "Data Quality Assessment, Data Modelling (Star Schema). \n",
      "    Databases:   \n",
      "MySQL, Oracle SQL (Oracle Database). \n",
      "    Analytics:  \n",
      " \n",
      "Statistical Analysis, EDA, Anomaly Detection, Hypothesis Testing, Business Intelligence. \n",
      " \n",
      "Projects \n",
      "Personalised Hotel Recommendation Analytics. \n",
      "  Python, Web Scraping, Exploratory Data Analysis, Statistical Modelling, Power BI Dashboard. GitHub \n",
      "• \n",
      "Addressed complex travel planning challenges caused by fluctuating hotel prices influenced by location, \n",
      "seasonality, and booking timing. \n",
      "• \n",
      "Developed scalable ETL pipelines for data collection and preprocessing using Python web scraping. \n",
      "• \n",
      "Performed exploratory data analysis (EDA) and statistical modelling to identify dynamic pricing patterns and \n",
      "seasonal Trends. \n",
      "• \n",
      "Generated actionable, data-driven recommendations leading to 25-35% cost savings and optimised travel budgeting. \n",
      "Relational Database Design & SQL Analysis on Music Store Sales. \n",
      "             \n",
      "MySQL, Advanced SQL, Data Modelling GitHub \n",
      "• \n",
      "Needed to extract actionable business insights from complex music sales data. \n",
      "• \n",
      "Design an efficient relational database schema and analyse sales and customer trends. \n",
      "• \n",
      "Developed and normalised 10+ relational tables, ensuring data integrity; wrote advanced SQL queries using window \n",
      "functions and CTEs for trend analysis; optimised query performance by 40%. \n",
      "• \n",
      "Identified key customer segments and sales patterns, generating $50K+ incremental revenue opportunities and enhancing \n",
      "marketing strategies. \n",
      "Telangana Climate-Scope - Power BI Climate Intelligence Dashboard. \n",
      " \n",
      "  Power BI, DAX, Time-Series Analysis | 2021–2024 Climate Data GitHub \n",
      "• \n",
      "Designed and implemented an interactive Power BI dashboard analysing district and mandal-level climate data for \n",
      "rainfall, temperature, humidity, and wind speed and applied advanced DAX functions and Power Query for time-series \n",
      "modelling, trend analysis, and seasonal anomaly detection. \n",
      "• \n",
      "Transformed complex climate datasets into actionable intelligence, enabling data-driven decision-making for \n",
      "stakeholders across agriculture, urban planning, and disaster management. \n",
      "• \n",
      "Improved operational efficiency and forecasting accuracy by providing real-time, accurate climate insights to over 50+ \n",
      "stakeholders. \n",
      " \n",
      "   Certification \n",
      "• \n",
      "NASSCOM FutureSkills Prime (Advanced Data Science with Python). Link \n",
      "Apr 2025 \n",
      "• \n",
      "Innomatics Research Labs: Python Programming & Exploratory Data Analysis (EDA). Link \n",
      "May 2025 \n",
      "• \n",
      "Innomatics Research Labs: SQL for Data Analysis and Power BI for Data Visualisation. Link \n",
      "Jul 2025 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1770458544349,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "XomenPQcixj9"
   },
   "outputs": [],
   "source": [
    "class DataFormat(TypedDict):\n",
    "  summary:str\n",
    "  experience:Optional[int]\n",
    "  skills:list[str]\n",
    "  links:Annotated[list[str],\"if any links found in the text return me the links as list of string make sure the link should be active\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLoDo0uFfPz5"
   },
   "source": [
    "This defines a typed dictionary schema for structured output where summary is a text overview, experience is optional, skills is a list of skills, and links is a list of valid, active URLs, with Annotated guiding the LLM to return only usable links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1770458547567,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "D4t2U0ymjOco"
   },
   "outputs": [],
   "source": [
    "final_model = model.with_structured_output(DataFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 20855,
     "status": "ok",
     "timestamp": 1770458569948,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "OpGaBUrxjbI-"
   },
   "outputs": [],
   "source": [
    "final_response = final_model.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1770458698661,
     "user": {
      "displayName": "Shrutika Kapade",
      "userId": "07496277854067889905"
     },
     "user_tz": -330
    },
    "id": "-KNtUSX5jqEZ",
    "outputId": "9d881cb5-a0c7-4b4a-a54c-ab4a2f876bdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Data Scientist skilled in Python, SQL, EDA, Machine Learning, and Data Visualisation with hands-on experience in end-to-end analytics workflows. Completed practical projects involving data cleaning, feature engineering, and predictive modelling using standard ML techniques. Currently working as an AI Agent Intern and Data Science and GenAI Intern at Innomatics Research Labs, contributing to workflow automation, data validation, and process improvement. Focused on applying data-driven problem-solving to deliver clear insights and support effective business decision-making.',\n",
       " 'experience': 0,\n",
       " 'skills': ['Python',\n",
       "  'Pandas',\n",
       "  'NumPy',\n",
       "  'Scikit-learn',\n",
       "  'SQL',\n",
       "  'Window Functions',\n",
       "  'CTEs',\n",
       "  'Optimisations',\n",
       "  'Power BI',\n",
       "  'Advanced DAX',\n",
       "  'Power Query',\n",
       "  'Excel',\n",
       "  'Power Pivot',\n",
       "  'VLOOKUP',\n",
       "  'Data Quality Assessment',\n",
       "  'Data Modelling',\n",
       "  'Star Schema',\n",
       "  'MySQL',\n",
       "  'Oracle SQL',\n",
       "  'Statistical Analysis',\n",
       "  'EDA',\n",
       "  'Anomaly Detection',\n",
       "  'Hypothesis Testing',\n",
       "  'Business Intelligence'],\n",
       " 'links': ['LinkedIn',\n",
       "  'GitHub',\n",
       "  'NASSCOM FutureSkills Prime Certification',\n",
       "  'Innomatics Research Labs Certification']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn1yTIii8TvL"
   },
   "source": [
    "### Why do we use Pydantic instead of TypedDict?\n",
    "- We use Pydantic because it provides runtime data validation, automatic type conversion, and detailed error handling, whereas TypedDict only defines structure and does not validate data at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFfhpBkY8cXY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPTg5Wr6VqDZZNwgLQxEst5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
